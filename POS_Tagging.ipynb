{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_Tagging.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimalkum/Hindi-POS-Tagger/blob/master/POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5I_hHtLzqVW",
        "colab_type": "code",
        "outputId": "38ca929c-c502-4a9c-cb83-fad614f55297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "training = open(\"/content/drive/My Drive/Colab Notebooks/hin_health_set02.txt\",\"r\").read()\n",
        "training=training.split('\\n')\n",
        "sentence,tags=[],[]\n",
        "#print(len(training))\n",
        "for i in range(0,len(training)-1):\n",
        "  sen,tag=[],[]\n",
        "  #print(training[i])\n",
        "  for word in training[i].split()[1:]:\n",
        "    w = word.split('//')\n",
        "    sen.append(w[0])\n",
        "    tag.append(w[1])\n",
        "    #print(w[0],w[1])\n",
        "  sentence.append(sen)\n",
        "  tags.append(tag)\n",
        "\n",
        "#print(sentence)\n",
        "\n",
        "training_data = []\n",
        "for sent,tag in zip(sentence,tags):\n",
        "  training_data.append((sent,tag))\n",
        "                       \n",
        "\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "#print(word_to_ix)\n",
        "tag_to_ix = {}\n",
        "ix_to_tag={}\n",
        "for sent, tags in training_data:\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            l=len(tag_to_ix)\n",
        "            tag_to_ix[tag] = l\n",
        "            ix_to_tag[l]=tag\n",
        "print(tag_to_ix)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'N_NN': 0, 'PSP': 1, 'N_NNP': 2, 'RD_PUNC': 3, 'CC_CCD': 4, 'V_VM': 5, 'V_VAUX': 6, 'RP_INTF': 7, 'QT_QTC': 8, 'DM_DMD': 9, 'RB': 10, 'N_NST': 11, 'QT_QTF': 12, 'RP_RPD': 13, 'JJ': 14, 'RP_NEG': 15, 'PR_PRL': 16, 'CC_CCS': 17, 'PR_PRP': 18, 'RD_UNK': 19, 'PR_PRQ': 20, 'DM_DMQ': 21, 'DM_DMI': 22, 'PR_PRF': 23, 'RD_ECH': 24, 'RD_RDF': 25, 'DM_DMR': 26, 'QT_QTO': 27, 'PR_PRI': 28, 'RD_SYM': 29, '/RD_PUNC': 30, 'PR_PRC': 31, 'DM_DRI': 32, 'RP_INJ': 33, '/RD_SYM': 34, 'CC_CSD': 35, 'NST': 36, 'PR_RRF': 37, 'PR_RPP': 38, 'RP_RPQ': 39, 'RP_PRD': 40, 'RD_PUN': 41, 'CCS': 42, 'RP_PUNC': 43, 'CC_CS': 44, 'PR_RPL': 45, 'RD_VAUX': 46, 'N_N_NN': 47, '_VM': 48, 'RD-PUNC': 49, 'परीक्षण': 50, 'Q_QTC': 51, 'NJJ': 52, 'V_V': 53, 'RD_': 54, 'PSP,': 55, 'RD_UPNC': 56, 'N_N': 57, 'RD_RPD': 58, 'RP_PRF': 59, 'V_VAUX।': 60, 'N_NN,': 61, 'C_CCS': 62, 'RP_RP_RPD': 63, 'MD_DMD': 64, 'N_NNP,': 65, 'V_RD_PUNC': 66, 'VAUX': 67, 'PR_NEG': 68, 'RD_PUNV': 69, 'RD_PUC': 70, 'NN': 71, 'QT_QT': 72, 'JJभोजन': 73, 'V_AUX': 74, 'V_VAM': 75, 'PR_PRO': 76, 'CCC_CSD': 77, 'vein': 78}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MRT-h6r0SFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 32\n",
        "HIDDEN_DIM = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2rWEFYf0g0H",
        "colab_type": "code",
        "outputId": "95bdb860-853c-4942-e013-8b9d25d23ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3006a25d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ivLOX00tjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,bidirectional=True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        lstm_out1, _ = self.lstm(embeds.view(len(sentence),1,-1))\n",
        "        tag_space = self.hidden2tag(lstm_out1.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL1AknF01FZn",
        "colab_type": "code",
        "outputId": "a5f89f01-c6dc-4f18-e89d-c26ee6b7175a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "training_data,testing_data = train_test_split(training_data,test_size=0.2)\n",
        "for epoch in range(20):  \n",
        "    for sentence, tags in training_data:\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "       \n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "       \n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "       \n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(testing_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "\n",
        "    tag_seq=''\n",
        "    print(len(tag_scores))\n",
        "    for i in range(0,len(tag_scores)):\n",
        "      m=max(tag_scores[i])\n",
        "      for j in range(0,len(tag_scores[i])):\n",
        "        if m==tag_scores[i][j]:\n",
        "          print(ix_to_tag[j],end='-')\n",
        "          tag_seq+=ix_to_tag[j]+'-'\n",
        "print('\\n')\n",
        "print(tag_seq.split('-'))\n",
        "print(testing_data[0][1])\n",
        "\n",
        "#print(tag_seq)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-4.1687, -4.5414, -4.4407,  ..., -4.2570, -4.4730, -4.3297],\n",
            "        [-4.3378, -4.4665, -4.3907,  ..., -4.3426, -4.4697, -4.3296],\n",
            "        [-4.2095, -4.4799, -4.3830,  ..., -4.3441, -4.5752, -4.3206],\n",
            "        ...,\n",
            "        [-4.0153, -4.3367, -4.3342,  ..., -4.2591, -4.5047, -4.3309],\n",
            "        [-4.1266, -4.3291, -4.3457,  ..., -4.2652, -4.4511, -4.3711],\n",
            "        [-4.1829, -4.3202, -4.3136,  ..., -4.4070, -4.4607, -4.4184]])\n",
            "13\n",
            "N_NN-PSP-N_NN-RP_NEG-V_VM-PSP-V_VM-N_NN-PSP-V_VM-V_VAUX-V_VAUX-RD_PUNC-\n",
            "\n",
            "['N_NN', 'PSP', 'N_NN', 'RP_NEG', 'V_VM', 'PSP', 'V_VM', 'N_NN', 'PSP', 'V_VM', 'V_VAUX', 'V_VAUX', 'RD_PUNC', '']\n",
            "['N_NN', 'PSP', 'N_NN', 'RP_NEG', 'V_VM', 'PSP', 'N_NN', 'N_NN', 'PSP', 'V_VM', 'V_VAUX', 'V_VAUX', 'RD_PUNC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhhEt1xZ3M3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}